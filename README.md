# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #1 выполнил(а):
- Панюкова Екатерина Алексеевна
- РИ210947
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы
Ознакомиться с основными операторами зыка Python на примере реализации линейной регрессии.

## Задание 1 Измените параметры файла .yaml-агента и определить какие параметры и как влияют на обучение модели.
Параметры по умолчанию. 
![image](https://user-images.githubusercontent.com/113353473/205339746-1f40f0a7-a13c-457a-90d7-59d3d714366b.png)
![image](https://user-images.githubusercontent.com/113353473/205341772-c7f4183c-1a27-44fe-ab5a-263877f05cea.png)


График Cumulative Reward возрастает монотонно верх:
![image](https://user-images.githubusercontent.com/113353473/205339234-2c8baba1-7dc2-4f23-be9a-0de71c9e6e39.png)
Параметр learning_rate = 1.0e-5 соответствуют начальной скорости обучения для градиентного спуска. График сначала возрастает, дальше убывает. То есть вознаграждение за обучение достигло максимума и начало уменьшаться:
![image](https://user-images.githubusercontent.com/113353473/205339340-9d8d830f-f48d-4a56-8936-551a801d0f6d.png)
Параметр beta = 1.0e-4 соответствует силе энтропийной регуляризации, которая делает политику «более случайной». Обучение поощерялось за константу на каждом шаге, график прямой:
![image](https://user-images.githubusercontent.com/113353473/205339407-09a647b8-0f9e-4fa7-8c3e-4d09dc2ba4dd.png)
Параметр epsilon = 0.3 влияет на то, насколько быстро политика может развиваться во время обучения. График растёт вверх линейно, но медленно
![image](https://user-images.githubusercontent.com/113353473/205339470-b679c14c-e3f7-4e35-9b1b-544478a946d5.png)
Параметр lambd = 0.9 используется при расчете обобщенной оценки преимущества (GAE). Сначала растёт, потом убывает. Обучение не стабильное:
![image](https://user-images.githubusercontent.com/113353473/205339523-09bacd39-0bda-44a8-bb8f-8d631449c2aa.png)
Параметр num_epoch = 10 - количество проходов через буфер опыта при выполнении оптимизации градиентного спуска. Логарифмический график функции, скачкообразный - не стабильные обновления:
![image](https://user-images.githubusercontent.com/113353473/205339577-a3cb86bb-8630-46e7-8530-b21d5319b460.png)

## Задание 2 Опишите результаты, выведенные в TensorBoard.
1)Environment
График Cumulative Reward: среднее совокупное вознаграждение за эпизод по всем агентам. Должно увеличиваться во время успешной тренировки.

График Episode Length: Средняя продолжительность каждого эпизода в окружающей среде для всех агентов. Во всех тренировках был одинаковый.
2)Losses
График Policy Loss: Средняя величина функции потерь. Соответствует тому, насколько сильно меняется политика (процесс принятия решений о действиях). Этот график должен уменьшаться во время успешной тренировки.

График Value Loss: Средняя потеря функции обновления значения. Соответствует тому, насколько хорошо модель способна предсказать значение каждого состояния. Этот график должен увеличиваться, пока агент учится, а затем уменьшаться, как только вознаграждение стабилизируется. Во всех тренировках был одинаковый.
3)Графики в разделе Policy показывают изменение некоторых параметров, которые указаны в .yaml файле. Во всех тренировках были одинаковые.
График Beta: гиперпараметр для настройки Entropy.
График Entropy: график случайности решений модели. Должен уменьшаться во время успешного эпизода.
График Epsilon: гиперпараметр, влияет на скорость развития политики.
График Extrinsic Reward: соответствует среднему совокупному вознаграждению, полученному от окружающей среды за эпизод.
График Extrinsic Value Estimate: это среднее значение, посещённое всеми состояниями агента. Чтобы отражать увеличение знаний агента, это значение должно расти, а затем стабилизироваться.
График Learning Rate: показывает величину шага при поиске оптимальной политики. Должен уменьшаться линейно.
4)График ELO: показывает силу сети.

С помощью графиков можно понять, какие параметры стоит изменить или оставить для лучшей тренировки модели.

## Выводы

В ходе работы осуществленно ознакомление с Python на примере организации линейной регрессии. В первом задании был написан код на языке Python для вывода фразы "Hello, World" в сервисе Google.colab и на языке C# в Visual Studio Code, который был запущен в Unity. Во втором задании осуществленна рвбота с кодом, реализующим линейную регрессию, были найдены закономерности: значения параметров a и b с каждой итерацией увеличиваются, а значения потерь уменьшаются. В третьем задании в ходе анализа кода сделаны выводы: 
1)Если уменьшить исходные данные, то значение потерь тоже уменьшается
2)параметр Lr отвечает за разницу значений после каждой итерации.

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
